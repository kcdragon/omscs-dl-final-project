{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bc271-165a-4d74-8f7d-8434ad9991d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012a5ea-5e24-4d61-9eee-af0c7e928d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blackbox Attack\n",
    "\n",
    "The blackbox attack works as follows:\n",
    "Since we do not know details about the model we are attacking, we train a \"substitute model\" to approximate the \"target model\".\n",
    "This is done using a small sample of training data, labeled by the target model (aka the oracle) instead of using the true labels. \n",
    "This way, the substitute learns to mimic the target model.\n",
    "In addition, we augment this training data using \"Jacobian-based data augmentation\" (TODO)\n",
    "Finally, we use one of the regular attacks on the substitute model. Supposedly, an attack that works well against the substitute\n",
    "will also work well against the target. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f478a7-7dd8-4d49-a5ca-c213a8ca572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd import grad\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import team36\n",
    "from team36.mnist.data_loading import MNIST_Loader\n",
    "from team36.mnist.vgg import VGG\n",
    "from team36.mnist.cnn import CNN\n",
    "from team36.attacks.fast_gradient_attack_data_set import FastSignGradientAttackDataSet\n",
    "from team36.defenses.fast_gradient_sign_method_loss import FastGradientSignMethodLoss\n",
    "from team36.training import train, validate, accuracy, predict, predict_from_loader, do_training, load_or_train, train_val_split, train_batch\n",
    "\n",
    "DIR = '.'\n",
    "DATA_DIR = f'{DIR}/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557589a6-1c27-4fe2-aa10-dc4e790cd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1\n",
    "Train the target model (aka the Oracle) or load from a checkpoint\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8988865-ab3e-46b5-a23c-b4e20a2231ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set up the datasets\n",
    "It is interesting to compare using the same dataset for the substitute and oracle vs. using different datasets\n",
    "\"\"\"\n",
    "ORACLE_DATASET = 'MNIST' # 'MNIST' or 'CIFAR10'\n",
    "SUB_DATASET = 'MNIST' \n",
    "DATASET_NAMES = [ORACLE_DATASET, SUB_DATASET]\n",
    "MODEL_NAMES = ['oracle', 'sub']\n",
    "ORACLE = 0\n",
    "SUB = 1\n",
    "datasets = [] # two-element list to store the oracle dataset and the substitute dataset\n",
    "dataset_image_sizes = []\n",
    "dataset_channels = []\n",
    "test_datasets = []\n",
    "\n",
    "# class Grayscale_to_RGB(object):\n",
    "#     def __call__(self, sample):\n",
    "#         print(sample)\n",
    "#         return sample\n",
    "\n",
    "# def grayscale_to_rgb(sample):\n",
    "#     print(sample)\n",
    "#     return sample\n",
    "# #     return sample.repeat(1, 3, 1, 1)\n",
    "\n",
    "for idx, dataset_name in enumerate(DATASET_NAMES):\n",
    "    transform_seq = [transforms.ToTensor()]\n",
    "    if dataset_name == 'MNIST':\n",
    "        image_size = 28\n",
    "        in_channels = 1\n",
    "        if idx == SUB and ORACLE_DATASET != 'MNIST': # if substitute uses mnist, but oracle uses other dataset add padding to match image size\n",
    "            padding = (dataset_image_sizes[ORACLE] - image_size) // 2\n",
    "            transform_seq.append(transforms.Pad(padding, fill=0))\n",
    "            image_size += padding * 2\n",
    "#             transform_seq.append(transforms.Lambda(lambda x: x))\n",
    "            transform_seq.append(transforms.Lambda(lambda x: x.repeat(3, 1, 1)))\n",
    "            in_channels = 3 # also need to copy to 3 channels to match rgb cifar\n",
    "            # NOTE: above assumes oracle_dataset has larger images and that there is an even number difference in the image sizes\n",
    "        dataset = torchvision.datasets.MNIST(root=DATA_DIR, train=True, download=True, \n",
    "                                              transform=transforms.Compose(transform_seq))\n",
    "        test_dataset = torchvision.datasets.MNIST(root=DATA_DIR, train=False, download=True, \n",
    "                                              transform=transforms.Compose(transform_seq))\n",
    "#         if idx == SUB and ORACLE_DATASET != 'MNIST':\n",
    "#             # also need to copy to image to 3 channels to match rgb cifar\n",
    "# #             transform_seq.append(grayscale_to_rgb) # copy single channel to 3 channels (from https://discuss.pytorch.org/t/grayscale-to-rgb-transform/18315)\n",
    "#             dataset.data = dataset.data.unsqueeze(1).repeat(1, 3, 1, 1)            \n",
    "#             in_channels = 3\n",
    "    elif dataset_name == 'CIFAR10':\n",
    "        image_size = 32\n",
    "        in_channels = 3\n",
    "        dataset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True,\n",
    "                                                transform=transforms.Compose(transform_seq))\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, \n",
    "                                              transform=transforms.Compose(transform_seq))\n",
    "    else:\n",
    "        print('ERROR: invalid dataset name', dataset_name)\n",
    "    datasets.append(dataset)\n",
    "    dataset_image_sizes.append(image_size)\n",
    "    dataset_channels.append(in_channels)\n",
    "    test_datasets.append(test_dataset)\n",
    "\n",
    "# optionally, use the test set to train the substitute, so that it trains on different data than the oracle:\n",
    "# datasets[SUB] = test_datasets[SUB]\n",
    "    \n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8a768-bb15-4d79-9677-4667566a2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_size = dataset_image_sizes[ORACLE]\n",
    "in_channels = dataset_channels[ORACLE]\n",
    "oracle = VGG(image_size=image_size, in_channels=in_channels) # set the target model here\n",
    "oracle_checkpoint = f'{DATASET_NAMES[ORACLE]}-vgg.pth' # e.g. 'MNIST-vgg.pth'\n",
    "# oracle_checkpoint = 'mnist-vgg.pth'\n",
    "oracle_checkpoint = oracle_checkpoint.lower()\n",
    "\n",
    "learning_rate = 5e-4\n",
    "momentum = 5e-1\n",
    "# momentum = .9\n",
    "weight_decay = 1e-1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "load_or_train(oracle, oracle_checkpoint, dataset=datasets[ORACLE], epochs=epochs, learning_rate=learning_rate, \n",
    "              weight_decay=weight_decay, momentum=momentum, batch_size=batch_size,\n",
    "             optim=torch.optim.SGD)\n",
    "# hyperparam tuning notes: \n",
    "# small learning rate is good for VGG on CIFAR (0.001 better than 0.01 and much better than 0.1)\n",
    "\n",
    "# if os.path.exists(oracle_checkpoint_path): # if trained checkpoint exists, load it\n",
    "#     state_dict = torch.load(f\"{DIR}/checkpoints/{checkpoint}\", map_location=torch.device('cpu'))\n",
    "#     model.load_state_dict(state_dict)\n",
    "# else: # else, train the model\n",
    "# training_indices, validation_indices, _, _ = train_test_split(\n",
    "#     range(len(target_data)),\n",
    "#     target_data.targets,\n",
    "#     stratify=target_data.targets,\n",
    "#     test_size=0.1,\n",
    "# )\n",
    "# oracle_train_split = torch.utils.data.Subset(target_data, training_indices)\n",
    "# oracle_val_split = torch.utils.data.Subset(target_data, validation_indices)\n",
    "# print(f\"{len(oracle_train_split)} in training set\")\n",
    "# print(f\"{len(oracle_val_split)} in validation set\")\n",
    "\n",
    "# do_training(oracle, training_split=oracle_train_split, validation_split=oracle_val_split, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f30d2-c77d-4b06-91d8-4d8e624bfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2\n",
    "Train the substitute model on a small portion of the training data, using the oracle's predictions as the labels\n",
    "\n",
    "oracle pred full mnist\n",
    "replace targets\n",
    "train val split (small train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de844c6c-5711-4ea6-8b67-413565b8ab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[SUB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06c595-4636-48ad-a49c-2bea49da1ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52fece1-9a25-4957-a734-a03affe7e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loader = torch.utils.data.DataLoader(datasets[SUB], batch_size=100, shuffle=False, num_workers=0)\n",
    "oracle_preds = predict_from_loader(oracle, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54401f17-407a-46cb-ba1a-7d6759d023c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_preds = oracle_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd89050-b754-4560-8baf-9d4e1f2c11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oracle_preds[oracle_preds != datasets[SUB].targets]) # check oracle preds mostly matches true labels (if oracle was trained on same dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51dceb-0107-4646-9380-d3c3c6ad9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[SUB].targets = oracle_preds # replace true labels with oracle's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad75d9-e327-4391-96f7-7c29bd15231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run either this cell (simple blackbox training) OR the next one (blackbox w/ jacobian-based data augmentation)\n",
    "torch.manual_seed(0)\n",
    "sub = CNN(image_size = dataset_image_sizes[SUB], in_channels = dataset_channels[SUB])\n",
    "sub_checkpoint = f'{DATASET_NAMES[SUB]}-substitute.pth' # e.g. 'MNIST-substitute.pth'\n",
    "\n",
    "loader = torch.utils.data.DataLoader(datasets[SUB], batch_size=100, shuffle=False, num_workers=0)\n",
    "# NOTE: due to a bug with lambda transforms on Windows, num_workers needs to be 0 or there will be an error\n",
    "# https://github.com/belskikh/kekas/issues/26\n",
    "\n",
    "epochs = 30\n",
    "# train_size = 10000\n",
    "train_size = 1800\n",
    "learning_rate = 0.05\n",
    "momentum = 0.5\n",
    "train = Subset(datasets[SUB], range(train_size))\n",
    "val = Subset(datasets[SUB], range(train_size, train_size*2)) # val split same size as train split\n",
    "load_or_train(sub, sub_checkpoint, train_split=train, val_split=val, epochs=epochs, learning_rate=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad07cd-f7d2-4690-86ac-baaa7a303aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train with Jacobian-based data augmentation\n",
    "# here, training starts with a small sample from the training data\n",
    "# we train on this over some number of \"sub_epochs\"\n",
    "# then, we augment the data by adding the sign of the Jacobian times a constant (or just the Jacobian times a constant?)\n",
    "# repeat over some number of epochs\n",
    "torch.manual_seed(0)\n",
    "epochs = 6\n",
    "sub_epochs = 10\n",
    "# train_size = 1000\n",
    "batch_size = 150\n",
    "step_size = .1\n",
    "learning_rate = 0.05\n",
    "momentum = 0.9\n",
    "# train = Subset(datasets[SUB], range(train_size))\n",
    "# val = Subset(datasets[SUB], range(train_size, train_size*2)) # val split same size as train split\n",
    "loader = torch.utils.data.DataLoader(test_datasets[SUB], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "batch_iter = iter(loader)\n",
    "X = batch_iter.next() # we just need one batch. result is: [[data], [targets]]\n",
    "for epoch in range(epochs):\n",
    "    sub = CNN(image_size = dataset_image_sizes[SUB], in_channels = dataset_channels[SUB]) # model is trained from scratch each time on the new augmented dataset\n",
    "    print('epoch', epoch)\n",
    "    print('len X', len(X[0]))\n",
    "#     train = Subset(datasets[SUB], range(epoch*batch_size, epoch*batch_size+batch_size))\n",
    "#     train = X\n",
    "#     print('training')\n",
    "    train_batch(X, sub, epochs=sub_epochs, learning_rate=learning_rate, momentum=momentum)\n",
    "#     print('done training')\n",
    "#     do_training(model, training_split=train, validation_split=train, epochs=sub_epochs) # train on batch \n",
    "    # generate the next batch using Jacobian data-augmentation\n",
    "#     print('augmentation')\n",
    "    X_size = len(X[0])\n",
    "    perm = torch.randperm(X_size)\n",
    "    X[0] = X[0][perm]\n",
    "    X[1] = X[1][perm]\n",
    "    sample_size = min(X_size, 400)\n",
    "    for i in range(sample_size):\n",
    "        x = X[0][i] # X[0] gets data from batch, [i] gets the i'th data point\n",
    "        x = x.unsqueeze(0) # need to pass a batch to the Jacobian, so we make a one-element batch \n",
    "#         print('getting jacobian')\n",
    "        J = jacobian(sub, x)\n",
    "        J = J.squeeze(0).squeeze(1) # switch from batch Jacobian back to single\n",
    "#         print('getting label')\n",
    "        label = oracle_preds[epoch*batch_size + i]\n",
    "#         print('label', label)\n",
    "        J = J[label] # only need jacobian for the corresponding label\n",
    "#         print('got J for label')\n",
    "#         X[0][i] += J # modify the input using the Jacobian\n",
    "        J = torch.sign(J) # only want the sign of the Jacobian\n",
    "#         J = J / torch.norm(J) # instead of taking the sign, we can move in the original direction with limited magnitude by normalizing\n",
    "        # don't just modify the examples X; modify and concat with the original examples to get a larger training set\n",
    "#         print('modifying x')\n",
    "        new_x = X[0][i] + step_size * J\n",
    "        new_x = new_x.unsqueeze(0) # make into a batch so we can cat\n",
    "#         print('catting new x')\n",
    "        X[0] = torch.cat((X[0], new_x), 0)\n",
    "#         X[0][i] = X[0][i] + step_size * J \n",
    "#     print('making new preds')\n",
    "    new_preds = torch.argmax(oracle(X[0]), axis=1) # get predictions for augmented dataset\n",
    "#     new_preds = torch.argmax(oracle(X[0][epoch*batch_size:(epoch+1)*batch_size]), axis=1) # get predictions for the new batch\n",
    "#     new_preds = new_preds.unsqueeze(0)\n",
    "    X[1] = new_preds\n",
    "#     step_size = (torch.pow(step_size, torch.tensor([epoch/epochs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1b84e-b6b0-41fb-9861-4539a1855da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = torch.utils.data.DataLoader(datasets[SUB], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "# batch_iter = iter(loader)\n",
    "# X = batch_iter.next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd52828-0cd9-4884-af25-44c8a1c0a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.argmax(oracle(X[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f237f9b-9cc5-4075-aa71-4b5ae48b6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oracle(X[0][1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5ff75-13e4-4170-bd9e-b6d7da00fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_iter = iter(loader)\n",
    "# X = batch_iter.next() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddca46-5f9b-4456-afbb-a825748f4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3\n",
    "Generate attack data for the substitute model and test both models with it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01ac1b-1aec-4121-b1d4-39c7788043de",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = oracle\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "models = [oracle]\n",
    "# models = [oracle, sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302e607-115f-42d9-b4bf-011b997f6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9eef07-8bd1-437f-b78d-af44471b76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Regular Test\"\"\"\n",
    "# test_set = torchvision.datasets.MNIST(root=DATA_DIR, train=False, download=True, \n",
    "#                                       transform=transforms.ToTensor())\n",
    "# MNIST: 98.83\n",
    "test_set = test_datasets[ORACLE] # since the substitute tries to mimic the oracle, we may as well test on the oracle's test set\n",
    "for model, name in zip(models, MODEL_NAMES):\n",
    "    print(name)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=0)\n",
    "\n",
    "    test_accuracy, _, test_loss = validate(None, test_loader, model, criterion)\n",
    "\n",
    "    print(f\"{name} Regular Test Accuracy is {test_accuracy}\")\n",
    "    print(f\"{name} Regular Test Loss is {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697795cf-818f-469c-8174-3b87234f943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Attack Test\"\"\"\n",
    "epsilon = 0.25\n",
    "test_set = test_datasets[ORACLE] # since the substitute tries to mimic the oracle, we may as well test on the oracle's test set\n",
    "attack_test_set = FastSignGradientAttackDataSet(test_set, sub, criterion, epsilon=epsilon)\n",
    "\n",
    "if DATASET_NAMES[ORACLE] == 'MNIST':\n",
    "    fgsm_defense_model = VGG()\n",
    "    fgsm_reg_defense_model = VGG()\n",
    "elif DATASET_NAMES[ORACLE] == 'CIFAR10':\n",
    "    fgsm_defense_model = VGG(image_size=32, in_channels=3)\n",
    "    fgsm_reg_defense_model = VGG(image_size=32, in_channels=3)\n",
    "    \n",
    "state_dict = torch.load(f\"{DIR}/checkpoints/{DATASET_NAMES[ORACLE].lower()}-vgg-training-with-fgsm-examples-defense-{epsilon}.pth\", map_location=torch.device('cpu'))\n",
    "fgsm_defense_model.load_state_dict(state_dict)\n",
    "state_dict = torch.load(f\"{DIR}/checkpoints/{DATASET_NAMES[ORACLE].lower()}-vgg-training-with-fgsm-regularization-defense-{epsilon}.pth\", \n",
    "                        map_location=torch.device('cpu'))\n",
    "fgsm_reg_defense_model.load_state_dict(state_dict)\n",
    "\n",
    "models = [(oracle, 'oracle'), \n",
    "          (fgsm_defense_model, 'FGSM Defense'), \n",
    "          (fgsm_reg_defense_model, 'FGSM Reg Defense')]\n",
    "# models = [oracle, sub]\n",
    "\n",
    "for model, name in models:\n",
    "    print(name)\n",
    "    test_loader = torch.utils.data.DataLoader(attack_test_set, batch_size=100, shuffle=False, num_workers=0)\n",
    "\n",
    "    test_accuracy, _, test_loss = validate(None, test_loader, model, criterion)\n",
    "\n",
    "    print(f\"{name} Attack Test Accuracy is {test_accuracy}\")\n",
    "    print(f\"{name} Attack Test Loss is {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba884cfd-8429-4ffb-af30-9b07a0cb4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = oracle\n",
    "index = 16\n",
    "# some clear images: 1=ship, 16=dog\n",
    "disp_width, disp_height = 200, 200\n",
    "\n",
    "input, _ = test_set[index]\n",
    "img = transforms.functional.to_pil_image(input)\n",
    "display(img.resize((disp_width, disp_height)))\n",
    "print(predict(model, input, soft=False))\n",
    "print(predict(model, input, soft=True))\n",
    "\n",
    "input, _ = attack_test_set[index]\n",
    "attack_img = transforms.functional.to_pil_image(input)\n",
    "display(attack_img.resize((disp_width, disp_height)))\n",
    "print(predict(model, input, soft=False))\n",
    "print(predict(model, input, soft=True))\n",
    "\n",
    "print('True label', test_set.targets[index])\n",
    "\n",
    "# input, _ = attack_test_set[index]\n",
    "# display(transforms.functional.to_pil_image(input))\n",
    "# print(predict(defense_model, input))\n",
    "\n",
    "# input, _ = attack_test_set[index]\n",
    "# display(transforms.functional.to_pil_image(input))\n",
    "# print(predict(fgsm_reg_defense_model, input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-final-project-cpu",
   "language": "python",
   "name": "cs7643-final-project-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
