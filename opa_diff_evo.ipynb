{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"opa_diff_evo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"cs7643-final-project-cpu","language":"python","name":"cs7643-final-project-cpu"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"cells":[{"cell_type":"code","metadata":{"id":"Da3IIAPDzvm8","executionInfo":{"status":"ok","timestamp":1627961200848,"user_tz":240,"elapsed":156,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"w_lzI-LWzvm8","executionInfo":{"status":"ok","timestamp":1627961201690,"user_tz":240,"elapsed":122,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"f3aec11a-c379-4d73-8666-a9e6f4ddc685"},"source":["import scipy\n","scipy.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.4.1'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlubAqYhzyXe","executionInfo":{"status":"ok","timestamp":1627961202753,"user_tz":240,"elapsed":175,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"4a64eac6-5b31-476b-9014-f8d757db272f"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvOfjiO9z3pX","executionInfo":{"status":"ok","timestamp":1627961204270,"user_tz":240,"elapsed":147,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"d7fe4029-69b6-40be-ee06-f334502fbb7e"},"source":["cd gdrive"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mBW3jlvz8Ey","executionInfo":{"status":"ok","timestamp":1627961204469,"user_tz":240,"elapsed":2,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"359d9ec2-50ea-4da8-84a7-00795d369c49"},"source":["cd MyDrive"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dN4KCe8Cz8MB","executionInfo":{"status":"ok","timestamp":1627961204874,"user_tz":240,"elapsed":4,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"3f742fd7-11aa-4656-ae8b-5ecaf146e626"},"source":["cd omscs-dl-final-project"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/omscs-dl-final-project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AxFh2KkAzvm9","executionInfo":{"status":"ok","timestamp":1627961238718,"user_tz":240,"elapsed":118,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["# import copy\n","# import matplotlib.pyplot as plt\n","# from sklearn.model_selection import train_test_split\n","# import torch\n","# import torch.nn as nn\n","# import torchvision\n","# import torchvision.transforms as transforms\n","# from team36.diff_evo import differential_evolution\n","# from team36.mnist.vgg import VGG\n","# from team36.attacks.fast_gradient_attack_data_set import FastSignGradientAttackDataSet\n","\n","# DIR = '.'\n","# DATA_DIR = f'{DIR}/data'\n","\n","# training_set = torchvision.datasets.MNIST(root=DATA_DIR, train=True, download=True, \n","#                                           transform=transforms.ToTensor())\n","\n","# prev_model = VGG()\n","# state_dict = torch.load(f\"{DIR}/checkpoints/mnist-vgg.pth\", map_location=torch.device('cpu'))\n","# print(prev_model.load_state_dict(state_dict))\n","# prev_criterion = nn.CrossEntropyLoss()\n","\n","# attack_training_set = FastSignGradientAttackDataSet(training_set, prev_model, prev_criterion, \n","#                                                     epsilon=0.25)\n","\n","\n","# indices = torch.randperm(len(attack_training_set))[:int(len(attack_training_set) * 0.1)]\n","# attack_training_set = torch.utils.data.Subset(attack_training_set, indices)\n","\n","# combined_training_set = torch.utils.data.ConcatDataset([training_set, attack_training_set])\n","\n","# training_indices, validation_indices = train_test_split(\n","#     range(len(combined_training_set)),\n","#     test_size=0.1,\n","# )\n","# training_split = torch.utils.data.Subset(combined_training_set, training_indices)\n","# validation_split = torch.utils.data.Subset(combined_training_set, validation_indices)\n","\n","# print(f\"{len(training_split)} in training set\")\n","# print(f\"{len(validation_split)} in validation set\")\n","\n","import copy\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Qfl9yAAzvm9","executionInfo":{"status":"ok","timestamp":1627953909120,"user_tz":240,"elapsed":355,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["import os\n","import sys\n","import numpy as np\n","\n","import argparse\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","\n","def perturb_image(xs, img):\n","    with torch.no_grad():\n","      # plt.figure()\n","      # plt.imshow(img[0][0].cpu().numpy())\n","      if xs.ndim < 2:\n","          xs = np.array([xs])\n","      # batch = len(xs)\n","      batch = 32\n","      imgs = img.repeat(batch, 1, 1, 1)\n","      xs = xs.astype(int)\n","\n","      count = 0\n","      for x in xs:\n","          pixels = np.split(x, len(x)/5)\n","          \n","          for pixel in pixels:\n","              x_pos, y_pos, r, g, b = pixel\n","              imgs[count, 0, x_pos, y_pos] = (r/255.0-0.4914)/0.2023\n","              imgs[count, 1, x_pos, y_pos] = (g/255.0-0.4822)/0.1994\n","              imgs[count, 2, x_pos, y_pos] = (b/255.0-0.4465)/0.2010\n","          count += 1\n","      # x, _ = imgs[0]\n","      display(transforms.functional.to_pil_image(imgs[0][0]))\n","      return imgs\n","\n","def predict_classes(xs, img, target_class, net, minimize=True):\n","    with torch.no_grad():\n","        imgs_perturbed = perturb_image(xs, img.clone())\n","    #     input = Variable(imgs_perturbed, volatile=True).cuda()\n","        predictions = net(imgs_perturbed).softmax(dim = 1).data.cpu().numpy()[:, target_class]\n","\n","        return predictions if minimize else 1 - predictions\n","\n","def attack_success(x, img, target_class, net, targeted_attack=False, verbose=False):\n","    with torch.no_grad():\n","        attack_image = perturb_image(x, img.clone())\n","    #     input = Variable(attack_image, volatile=True).cuda()\n","        confidence = net(attack_image).softmax(dim = 1).data.cpu().numpy()[0]\n","        predicted_class = np.argmax(confidence)\n","\n","        if (verbose):\n","            print(\"Confidence: %.4f\"%confidence[target_class])\n","        if (targeted_attack and predicted_class == target_class) or (not targeted_attack and predicted_class != target_class):\n","            return True\n","\n","\n","def attack(img, label, net, target=None, pixels=1, maxiter=75, popsize=400, verbose=False):\n","    # img: 1*3*W*H tensor\n","    # label: a number\n","    # plt.figure()\n","    # plt.imshow(img[0][0].cpu().numpy())\n","    # print(label)\n","    with torch.no_grad():\n","        targeted_attack = target is not None\n","        target_class = target if targeted_attack else label\n","\n","        bounds = [(0,32), (0,32), (0,255), (0,255), (0,255)] * pixels\n","\n","        popmul = max(1, popsize/len(bounds))\n","\n","        predict_fn = lambda xs: predict_classes(\n","            xs, img, target_class, net, target is None)\n","        callback_fn = lambda x, convergence: attack_success(\n","            x, img, target_class, net, targeted_attack, verbose)\n","\n","        inits = np.zeros([int(popmul)*int(len(bounds)), int(len(bounds))])\n","        for init in inits:\n","            for i in range(pixels):\n","                init[i*5+0] = np.random.random()*32\n","                init[i*5+1] = np.random.random()*32\n","                init[i*5+2] = np.random.normal(128,127)\n","                init[i*5+3] = np.random.normal(128,127)\n","                init[i*5+4] = np.random.normal(128,127)\n","\n","        # print('starting differential_evolution')\n","        attack_result = differential_evolution(predict_fn, bounds, maxiter=maxiter, popsize=popmul,\n","            recombination=1, atol=-1, callback=callback_fn, polish=False, init=inits)\n","        # print(\"finished differential_evolution\")\n","        attack_image = perturb_image(attack_result.x, img)\n","        # return (attack_image, label)\n","        # print(attack_image)\n","#     #     attack_var = Variable(attack_image, volatile=True).CUDA()\n","        predicted_probs = net(attack_image).softmax(dim = 1).data.cpu().numpy()[0]\n","\n","        predicted_class = np.argmax(predicted_probs)\n","\n","        if (not targeted_attack and predicted_class != label) or (targeted_attack and predicted_class == target_class):\n","            # return 1, attack_result.x.astype(int)\n","            return 1, attack_image\n","        # return 0, [None]\n","        return 0, attack_image\n","\n","\n","def attack_all(net, loader, pixels=1, targeted=False, maxiter=75, popsize=400, verbose=False):\n","\n","    correct = 0\n","    \n","    success = 0\n","    imges = []\n","    # print(len(loader.dataset))\n","    with torch.no_grad():\n","        for batch_idx, (input, target) in enumerate(loader):\n","            # plt.figure()\n","            # plt.imshow(input[0][0].cpu().numpy())\n","            display(transforms.functional.to_pil_image(input[0][0]))\n","            input = input.cuda()\n","            target = target.cuda()\n","            prior_probs = net(input).softmax(dim = 1)\n","            _, indices = torch.max(prior_probs, 1)\n","\n","            if target[0] != indices.data.cuda()[0]:\n","                continue\n","\n","            correct += 1\n","            target = target.cpu().numpy()\n","\n","            targets = [None] if not targeted else range(10)\n","\n","            for target_class in targets:\n","                if (targeted):\n","                    if (target_class == target[0]):\n","                        continue\n","                # target_class = 1\n","                flag, x = attack(input, target[0], net, target_class, pixels=pixels, maxiter=maxiter, popsize=popsize, verbose=verbose)\n","\n","                imges.append(attack_img)\n","            # plt.imshow(attack_img.cpu().numpy(), cmap='gray')\n","            # imges[batch_idx] = (x,target)\n","                # success += flag\n","                # if (targeted):\n","                #     success_rate = float(success)/(9*correct)\n","                # else:\n","                #     success_rate = float(success)/correct\n","\n","\n","                # if flag == 1:\n","                    # print(\"success rate: %.4f (%d/%d) [(x,y) = (%d,%d) and (R,G,B)=(%d,%d,%d)]\"%(\n","                        # success_rate, success, correct, x[0],x[1],x[2],x[3],x[4]))\n","\n","            # if correct == args.samples:\n","            #     break\n","    # return success_rate\n","    # return x, target\n","    return imges\n"],"execution_count":168,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PEm3oGC4zvm_","executionInfo":{"status":"ok","timestamp":1627961236875,"user_tz":240,"elapsed":4314,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"2457f9a2-a209-4a0f-a6b8-07a0d7057043"},"source":["\n","import os\n","import sys\n","import numpy as np\n","\n","import argparse\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","# from models import *\n","from team36.util import progress_bar\n","from torch.autograd import Variable\n","torch.cuda.empty_cache()\n","print(\"==> Loading data and model...\")\n","\n","DIR = '.'\n","DATA_DIR = f'{DIR}/data'\n","NAME = 'cifar10'\n","dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","training_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, \n","                                            transform=transforms.ToTensor())\n","\n","training_indices, validation_indices, _, _ = train_test_split(\n","    range(len(training_set)),\n","    training_set.targets,\n","    stratify=training_set.targets,\n","    test_size=0.1,\n",")\n","training_split = torch.utils.data.Subset(training_set, training_indices)\n","validation_split = torch.utils.data.Subset(training_set, validation_indices)\n","\n","print(f\"{len(training_split)} in training set\")\n","print(f\"{len(validation_split)} in validation set\")\n","\n","\n","import team36\n","from team36.diff_evo import differential_evolution\n","from team36.mnist.vgg import VGG\n","from team36.training import train, validate\n","\n","learning_rate = 1e-3\n","momentum = 5e-1\n","weight_decay = 5e-2\n","batch_size = 32\n","epochs = 1\n","\n","sampler = torch.utils.data.RandomSampler(training_split, replacement=True, num_samples=1000)\n","training_loader = torch.utils.data.DataLoader(training_split, batch_size=batch_size, sampler=sampler)\n","test_loader = torch.utils.data.DataLoader(validation_split, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","model = VGG(image_size=32, in_channels=3)\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","state_dict = torch.load(f\"{DIR}/checkpoints/cifar10-vgg.pth\", map_location=torch.device('cuda'))\n","criterion = nn.CrossEntropyLoss().cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n","                            momentum=momentum, weight_decay=weight_decay)\n","\n","\n","print(\"==> Starting attck...\")\n","pixels = 1\n","maxiter = 100 \n","popsize = 200\n","verbose = False\n","targeted = False\n","# results = attack_all(model, test_loader, pixels=pixels, targeted=targeted, maxiter=maxiter, popsize=popsize, verbose=verbose)\n","# print(\"Final success rate: %.4f\"%results)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["==> Loading data and model...\n","Files already downloaded and verified\n","45000 in training set\n","5000 in validation set\n","==> Starting attck...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":539},"id":"V_VfdmXz-RYO","executionInfo":{"status":"error","timestamp":1627948496220,"user_tz":240,"elapsed":2100,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"09bc0f0c-3b6c-459b-d19a-9d762a3b4bf2"},"source":["import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets, transforms\n","\n","attack_test_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, \n","                                             transform=transforms.ToTensor())\n","attack_test_set = attack_all(model, test_loader, pixels=pixels, targeted=targeted, maxiter=maxiter, popsize=popsize, verbose=verbose)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACpklEQVR4nDWTS3OUVRRF1z73fulOPwKERFEpxEdRSEY6obScOvJ/+ifUiU4sX4US8YGUCKXVKAGhk066+7tnO+hieia76qy19GE3jNoNuigXhjFgOClbo0HXFfB6tQ70ictWYVDsLopSkRFIDlDLyHo51SWQKigSFwkkm0aaGsgyhIBmW2GTAJlqVEAGY6S0cENsLn0EFcuASFyKDWkpIm1vOalIRnIfpW9VAYFwquDwZgLAkbIbAUShgSl2qCKw8rf+1XEJIlJgJIwJVFFA3v2258LB66NGZ4WxEalI1UDozqfd4HD+z40rz7q3RkpkAAmoYVz/vb996eDpyRejS6MnV15XGIAgocqIq2/8OHvt4tnR0WqPxWRvXRCALWqa4isfr3+dHQ270VPFE96frk/rsJAGV4CMa9dnp3l6ejIt83r37Nr4+933dgzixR+OfpH6sjhh7HH/2TeXn1+YvisCUwFHfXhrEA2viufLfvb40Tvty52r7gIHiPSTxXRr1crwWPur+bItZsf3P38khRS2nLn0wY1+7fr0j75bupT5Ufvzu7XIjJDJ2azdmZeTs1YWx2WRy1UZn68/3W7N1LDj5Ksf+rvPnMuIWgf745Unr8z6cmvrYEC1mT+4N/N21lHvHAxuvnxv1l9/8+vft99+fDJwzcXfPz98fnFYtmjrdQ5u3Ny5dqYhu389/2BZTV3cvn1kdkZSKter6cG5NhyTjMeLWpXUw8P/UmUy2p+W0/u5uvxSAFR5tI0x9fCYbrJ7aW9vGssH6s6fCxwUuVEyoZ7bmVzcn2xv1WC8G270kkk2qKB+RNcVp4TCiZwhSF7EUbfdmihhy5JobYNvIxXUlkKxKceb6DaubGhTO7AEIUSQ2uADE0Kum5Y2CmKSRKaJhoXyf7lxek7cIZYRAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7FE3D2C95190>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACnklEQVR4nGWOPW9cZRSEZ84593N914oRJAaUBqVIAy0SRX4JP4+an4EEFBESQiSKKEJIDHZir3f33vt+nEOBRZFMOc/o0fCnIN6LexBwRiJoHu9zlDQutVUv1uxp/HCgyFi5Fjnx3NsHGNBuHmfrwXUxGgAE8P8RAo4tx6+6VV5c2NbuyrsECbjMt4/lsPC+Xf1nABFgAARBQozPY/rh6TePxiJ3YpICUghIo83v3y/Kn5/ees8fXRAgCTAYDCoRf7z7rXz7Fx4WswinAKpRAA+SHiIPhu+uH3+ZMqqRhKumRTedlBopIsLuy9dvdv+ceENTkhXRTZuugQvWXKo1EU/etl2EqKmQVrrzKOsaRoFBNrsF0xy1bRqaOcXyuGPnFa5RQ2OJ1j+aLucgxRYh86THBh5SKz2IBRYhZzc1WSNv2wZ2vTf68bgsS/F0KA53oJ1sjSqD3DuTVwfJuR6WshyX4imzpae8PzDCpv1NjJ9OLCs6P6Sy3e5g0LbWm4O2ARMztpl9WZuTP3dalyi5t323lvmwBSAeZPJibE+e/3qbu/Ly6niRpSyvX11GE24UgfTpqrTPfvn8C6S+1DIft/Pry7kfWhejCKG3+9PrF8P5prSglNNTW+vpfhpImrqiahybv+38Y2hQYKKun+0uQkhauDLC6r4/f2iVjRBw3Kbtnp2Zi5EeyGWjPo4+SJAhXPfsRx3HBiIB9/DQQcaAk7U6rdoG/WBmaqYRDGlCVaMmSgDGorF4EykYZvBgiB+XMcXsSnojwIqCNCeqtfTKdRkunz160DVQUVHsu6RyNjCz2lyi+GG+uv4EcxXxGta30qFGyie7sbeIoG2HMpl7DrImR/C8vCk5l5havnTQJDwXh7vDq5c8z9nONnTr9F+W15uV7xKUqwAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7FE3D2C95410>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACpUlEQVR4nGWOy4odZRSF19p71+XU6TpNWjRplUwkg0wiOBIc+CQ+nmMfQ/ACQRAxITiIMYl2x+7TdU5V/Ze9HaQRjN/wW4vF4vdBvIN7EHBGImge7+YoaVhqq16smWj8f0GRsXItcuK5l7fux/8UurWl7c7u+NFoABD47N+UgGPH4dNulWevbWe38pYgAZf55qEcFt61y7cLIAIMgCBIiPFpjN8+/uLBUG4/kKSAFALSaPPrN4vyh8c33vM7FwRIAgwGg0rEb3//Ur76A/eLWYRTANUogAdJD5F7m6+vHj5KGdVIwlXTottOSo0UEWF35fNX+79OvKEpyYroxm3XwAVrLtWaiC/ftF2EqKmQVrrzKOsaRoFBtvsF4xy1bRqaOcXysGfnFa5RQ2OJ1t8bL+YgxRYh86jHBh5SKz2IBRYhZ9c1WSNv2gZ2NRn9eFyWpXg6FIc70I62RpWN3DmTFwfJuR6WshyX4imzpac8HRhh43Qdw4cjy4rOD6nsdnsYtK31+qBtwMSMbWZf1ubk973WJUruberWMh92AMSDTF6M7cnTn29yV55fHl9nKcvLFxfRhBtFIH26LO2Tnz7+BKkvtczH3fzyYu43rYtRhNCb6fTq2eZ8W1pQyumprfV0GjckTV1RNY7Nn3b+PjQoMFHXj/avQ0hauDLC6tSf37fKRgg4btJuYmfmYqQHctmqD4NvJMgQrhP7QYehgUjAPTx0I0PAyVqdVm2LfmNmaqYRDGlCVaMmSgDGorF4EykYZvBgiB+XIcXsSnojwIqCNCeqtfTKddlcPHlwr2ugoqKYuqRytmFmtblE8cN8efUB5iriNaxvpUONlE/2Q28RQdttymjuOciaHMHz8qrkXGJs+dxBk/BcHO4Or17yPGc729Kt038ATACbxD3NC6gAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7FE40448CB10>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACoElEQVR4nGWOy25cZRCEq7r7XMczFkaQGJA3KItsYIvEgifh8VjzGEjAIkJCiEQREiEkBjvY4zNzzvkv3SywWCS1rK/0qfhDEG/FPQg4IxE0j7c5ShqX2qoXayYa3x0oMlauRU489/YOBrSbx9l6cF2MBgAB/H+EgGPH8fNuleeXtrP78j5BAi7z3WM5LHxg1/8ZQAQYAEGQEOOz2H735MtHY5F7MUkBKQSk0ebXbxflj0/uvOf3LgiQBBgMBpWI3/75pXz9Jy6KWYRTANUogAdJD5GHwzc3jz9LGdVIwlXToptOSo0UEWEP5IvX+79PvKEpyYrotpuugQvWXKo1EV+9absIUVMhrXTnUdY1jAKDbPYLtnPUtmlo5hTL456dV7hGDY0lWn9/ezUHKbYImbd6bOAhtdKDWGARcnZbkzXypm1gN5PRj8dlWYqnQ3G4A+3W1qgyyHtn8vIgOdfDUpbjUjxltvSUpwMjbDvdxvjRlmVF54dUdrs9DNrWenvQNmBixjazL2tz8sde6xIl9zZ1a5kPOwDiQSYvxvbk2c93uSsvro+XWcry6uVVNOFGEUifrkv79KdPPkXqSy3zcTe/upr7oXUxihB6N53ePB/ON6UFpZye2lpPp+1A0tQVVePY/GXnH0CDAhN1/Xh/GULSwpURVqf+/MIqGyHguEu7iZ2Zi5EeyGWjPo4+yO8XDOE6sR91HBuIBNzDQwcZA86LWp1WbYN+MDM10wiGNKGqURMlAGPRWLyJFAwzeDDEj8uYYnYlvRFgRUGaE9VaeuW6DFdPHz3sGqioKKYuqZwNzKw2lyh+mK9vPsRcRbyG9a10qJHyyX7sLSJou6FszT0HWZMjeF5el5xLbFu+cNAkPBeHu8OrlzzP2c42dOv0X68Vm7KFhwwZAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7FE40448CED0>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACpElEQVR4nGWOu44cZRSEq845fZme7VmxCOwFZALkwAmkSASEPAWPR8xLICEBgYWEELYsAmPsFbv27mzPdPd/OYeAFYFdYX2lT8Wfg3gr7kHAGYmgebzNUdKw1Fa9WDPR+O5AkbFyLXLiubd3MKDdPMzWg+tiNAAI4P8jBBw7Dl90qzy7sJ3dlXcJEnCZbx/JYeE9u/rPACLAAAiChBifxvjj468eDkXuxCQFpBCQRps/vl+Uvzy+9Z4/uSBAEmAwGFQi/nzze/n2bzwoZhFOAVSjAB4kPUTub767fvR5yqhGEq6aFt12UmqkiAi7J1++2v9z4g1NSVZEN267Bi5Yc6nWRHz9uu0iRE2FtNKdR1nXMAoMst0vGOeobdPQzCmWhz07r3CNGhpLtP7+eDkHKbYImUc9NvCQWulBLLAIObupyRp53Taw68nox+OyLMXToTjcgXa0Naps5L0zeXGQnOthKctxKZ4yW3rK04ERNk43MXw0sqzo/JDKbreHQdtabw7aBkzM2Gb2ZW1O/tprXaLk3qZuLfNhB0A8yOTF2J48/e02d+X51fEiS1levriMJtwoAunTVWmf/PrJZ0h9qWU+7uaXl3O/aV2MIoTeTqfXzzbn29KCUk5Pba2n07ghKSoqIXE8XtinH0D1h0b7QV0/PrUQkhaujLA69ecPrLL5hoDjNu0mdmYuRnogl636MPhGggzhOrEfdBgaiATcw0M3MgScrNVp1bboN2amZhrBkCZUNWqiBGAsGos3kYJhBg+G+HEZUsyupDcCrChIc6JaS69cl83lk4f3uwYqKoqpSypnG2ZWm0sUP8xX1x9iriJew/pWOtRI+WQ/9BYRtN2mjOaeg6zJETwvr0rOJcaWzx00Cc/F4e7w6iXPc7azLd06/ReHeZnm8XzgIwAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7FE40448CC10>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACpElEQVR4nGWOu44cZRSEq845fZme7VmxCOwFZALkwAmkSASEPAWPR8xLICEBgYWEELYsAmPsFbv27mzPdPd/OYeAFYFdYX2lT8Wfg3gr7kHAGYmgebzNUdKw1Fa9WDPR+O5AkbFyLXLiubd3MKDdPMzWg+tiNAAI4P8jBBw7Dl90qzy7sJ3dlXcJEnCZbx/JYeE9u/rPACLAAAiChBifxvjj468eDkXuxCQFpBCQRps/vl+Uvzy+9Z4/uSBAEmAwGFQi/nzze/n2bzwoZhFOAVSjAB4kPUTub767fvR5yqhGEq6aFt12UmqkiAi7J1++2v9z4g1NSVZEN267Bi5Yc6nWRHz9uu0iRE2FtNKdR1nXMAoMst0vGOeobdPQzCmWhz07r3CNGhpLtP7+eDkHKbYImUc9NvCQWulBLLAIObupyRp53Taw68nox+OyLMXToTjcgXa0Naps5L0zeXGQnOthKctxKZ4yW3rK04ERNk43MXw0sqzo/JDKbreHQdtabw7aBkzM2Gb2ZW1O/tprXaLk3qZuLfNhB0A8yOTF2J48/e02d+X51fEiS1levriMJtwoAunTVWmf/PrJZ0h9qWU+7uaXl3O/aV2MIoTeTqfXzzbn29KCUk5Pba2n07ghKSoqIXE8XtinH0D1h0b7QV0/PrUQkhaujLA69ecPrLL5hoDjNu0mdmYuRnogl636MPhGggzhOrEfdBgaiATcw0M3MgScrNVp1bboN2amZhrBkCZUNWqiBGAsGos3kYJhBg+G+HEZUsyupDcCrChIc6JaS69cl83lk4f3uwYqKoqpSypnG2ZWm0sUP8xX1x9iriJew/pWOtRI+WQ/9BYRtN2mjOaeg6zJETwvr0rOJcaWzx00Cc/F4e7w6iXPc7azLd06/ReHeZnm8XzgIwAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7FE40448C450>"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-160-10d16f1bdb3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m attack_test_set = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, \n\u001b[1;32m      6\u001b[0m                                              transform=transforms.ToTensor())\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mattack_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargeted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargeted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-158-c3f19648e371>\u001b[0m in \u001b[0;36mattack_all\u001b[0;34m(net, loader, pixels, targeted, maxiter, popsize, verbose)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# imges.append(attack_img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# plt.imshow(attack_img.cpu().numpy(), cmap='gray')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mimges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0;31m# success += flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;31m# if (targeted):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"]}]},{"cell_type":"code","metadata":{"id":"OXjtryjk-qhb"},"source":["attack_test_loader = torch.utils.data.DataLoader(attack_test_set, batch_size=157, \n","                                                 shuffle=False, num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"id":"mK3QxnYp2xig","executionInfo":{"status":"error","timestamp":1627942670442,"user_tz":240,"elapsed":178,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"85b05e3b-32d4-4bd7-f963-83cfcc32a949"},"source":["attack_test_accuracy, _, attack_test_loss = validate(None, attack_test_loader, model, criterion)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-118-5e150c3f5176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattack_test_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/gdrive/MyDrive/omscs-dl-final-project/team36/training.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(epoch, val_loader, model, criterion, no_grad)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mnum_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","metadata":{"id":"Df-_bCxf6iYR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6U6YD_vnZr-","executionInfo":{"status":"ok","timestamp":1627961210974,"user_tz":240,"elapsed":921,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["import os\n","import sys\n","\n","import copy\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","DIR = \"/content/gdrive/MyDrive/omscs-dl-final-project\"\n","sys.path.append(os.path.abspath(DIR))\n","DATA_DIR = f'{DIR}/data'\n","\n","from team36.mnist.vgg import VGG\n","from team36.diff_evo import differential_evolution\n","from team36.training import predict"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"cyD9O5iEnZug","executionInfo":{"status":"ok","timestamp":1627960856615,"user_tz":240,"elapsed":911,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["NAME = \"cifar10\"\n","DATA_SET_CLASS = torchvision.datasets.CIFAR10\n","image_size = 32\n","in_channels = 3\n","fgsm_alpha = 0.5\n","fgsm_epsilon = 0.01\n","\n","model = VGG(image_size=image_size, in_channels=in_channels)\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","state_dict = torch.load(f\"{DIR}/checkpoints/{NAME}-vgg.pth\")\n","model.load_state_dict(state_dict)\n","\n","criterion = nn.CrossEntropyLoss()"],"execution_count":186,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihwToOsynZw-","executionInfo":{"status":"ok","timestamp":1627960857764,"user_tz":240,"elapsed":1151,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"34576943-c0d2-475b-ef24-3af24a722249"},"source":["class OnePixelAttackDataSet(torch.utils.data.Dataset):\n","    def __init__(self, baseline_dataset, model, device=None):\n","        self.baseline_dataset = baseline_dataset\n","        self.model = model\n","        self.device = device\n","\n","    def __getitem__(self, index):\n","        input, target = self.baseline_dataset[index]\n","\n","        if self.device is not None:\n","            input = input.to(self.device)\n","            \n","        inputs = input.unsqueeze(0)\n","        \n","        flag, adversarial_input = self.attack(input, target, self.model)\n","\n","        return adversarial_input, target\n","\n","    def __len__(self):\n","        return len(self.baseline_dataset)\n","\n","    def attack(self, img, label, net, target=None, pixels=1, maxiter=75, popsize=400, verbose=False):\n","      image_size = 32\n","\n","      # img: 1*3*W*H tensor\n","      # label: a number\n","      # plt.figure()\n","      # plt.imshow(img[0][0].cpu().numpy())\n","      # print(label)\n","      with torch.no_grad():\n","          targeted_attack = target is not None\n","          target_class = target if targeted_attack else label\n","\n","          bounds = [(0,image_size), (0,image_size), (0,255), (0,255), (0,255)] * pixels\n","\n","          popmul = max(1, popsize/len(bounds))\n","\n","          predict_fn = lambda xs: self.predict_classes(xs, img, target_class, net, target is None)\n","          callback_fn = lambda x, convergence: self.attack_success(\n","              x, img, target_class, net, targeted_attack, verbose)\n","\n","          inits = np.zeros([int(popmul)*int(len(bounds)), int(len(bounds))])\n","          for init in inits:\n","              for i in range(pixels):\n","                  init[i*5+0] = np.random.random()*image_size\n","                  init[i*5+1] = np.random.random()*image_size\n","                  init[i*5+2] = np.random.normal(128,127)\n","                  init[i*5+3] = np.random.normal(128,127)\n","                  init[i*5+4] = np.random.normal(128,127)\n","\n","          # print('starting differential_evolution')\n","          attack_result = differential_evolution(predict_fn, bounds, maxiter=maxiter, popsize=popmul,\n","              recombination=1, atol=-1, callback=callback_fn, polish=False, init=inits)\n","          # print(\"finished differential_evolution\")\n","          attack_image = self.perturb_image(attack_result.x, img)\n","          # return (attack_image, label)\n","          # print(attack_image)\n","          # attack_var = Variable(attack_image, volatile=True).CUDA()\n","          predicted_probs = net(attack_image).softmax(dim = 1).data.cpu().numpy()[0]\n","\n","          predicted_class = np.argmax(predicted_probs)\n","\n","          x = (not targeted_attack and predicted_class != label)\n","          y = (targeted_attack and predicted_class == target_class)\n","          if x or y:\n","              # return 1, attack_result.x.astype(int)\n","              return 1, attack_image\n","          # return 0, [None]\n","          return 0, attack_image\n","\n","    def predict_classes(self, xs, img, target_class, net, minimize=True):\n","        with torch.no_grad():\n","          imgs_perturbed = self.perturb_image(xs, img.clone())\n","      #     input = Variable(imgs_perturbed, volatile=True).cuda()\n","          predictions = net(imgs_perturbed).softmax(dim = 1).data.cpu().numpy()[:, target_class]\n","\n","          return predictions if minimize else 1 - predictions\n","\n","    def attack_success(self, x, img, target_class, net, targeted_attack=False, verbose=False):\n","      with torch.no_grad():\n","        attack_image = self.perturb_image(x, img.clone())\n","    #     input = Variable(attack_image, volatile=True).cuda()\n","        confidence = net(attack_image).softmax(dim = 1).data.cpu().numpy()[0]\n","        predicted_class = np.argmax(confidence)\n","\n","        if (verbose):\n","          print(\"Confidence: %.4f\"%confidence[target_class])\n","        cond_a = (targeted_attack and predicted_class == target_class)\n","        cond_b = (not targeted_attack and predicted_class != target_class)\n","        if cond_a or cond_b:\n","          return True\n","\n","    def perturb_image(self, xs, img):\n","      with torch.no_grad():\n","        # plt.figure()\n","        # plt.imshow(img[0][0].cpu().numpy())\n","        if xs.ndim < 2:\n","            xs = np.array([xs])\n","        batch = len(xs)\n","        # batch = 32\n","        imgs = img.repeat(batch, 1, 1, 1)\n","        xs = xs.astype(int)\n","\n","        count = 0\n","        for x in xs:\n","            pixels = np.split(x, len(x)/5)\n","            \n","            for pixel in pixels:\n","                x_pos, y_pos, r, g, b = pixel\n","                # print(\"x_pos, y_pos, r, g, b\", x_pos, y_pos, r, g, b)\n","                imgs[count, 0, x_pos, y_pos] = (r/255.0-0.4914)/0.2023\n","                imgs[count, 1, x_pos, y_pos] = (g/255.0-0.4822)/0.1994\n","                imgs[count, 2, x_pos, y_pos] = (b/255.0-0.4465)/0.2010\n","            count += 1\n","        # x, _ = imgs[0]\n","        # display(transforms.functional.to_pil_image(imgs[0][0]))\n","        return imgs\n","\n","test_set = DATA_SET_CLASS(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())\n","attack_test_set = OnePixelAttackDataSet(test_set, model, device='cuda')"],"execution_count":187,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"id":"MFI7ZP6znZzF","executionInfo":{"status":"ok","timestamp":1627960857952,"user_tz":240,"elapsed":191,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"99758caa-a445-4673-ad2f-fa1e59fce9a0"},"source":["index = 0\n","\n","input, ground_truth = test_set[index]\n","\n","adversarial_input, _ = attack_test_set[index]\n","adversarial_input = adversarial_input[0]\n","\n","print(\"input\", input.shape)\n","print(\"adversarial_input\", adversarial_input.shape)\n","\n","print(\"Ground truth is\", ground_truth)\n","\n","print(\"Test Image prediction is\", predict(model, input.cuda()))\n","display(transforms.functional.to_pil_image(input))\n","\n","print(\"Adversarial Test Image prediction is\", predict(model, adversarial_input.cuda()))\n","display(transforms.functional.to_pil_image(adversarial_input))"],"execution_count":188,"outputs":[{"output_type":"stream","text":["input torch.Size([3, 32, 32])\n","adversarial_input torch.Size([3, 32, 32])\n","Ground truth is 3\n","Test Image prediction is 3\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE4058BA9D0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Adversarial Test Image prediction is 5\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE405898910>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8eZDWY4nZ1p","executionInfo":{"status":"ok","timestamp":1627960892877,"user_tz":240,"elapsed":34077,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"18637638-d4ca-4a30-cd28-a1390424f36b"},"source":["count = 0\n","attack_count = 0\n","total = 100\n","for i in range(total):\n","  input, ground_truth = test_set[index]\n","  adversarial_input, _ = attack_test_set[index]\n","  adversarial_input = adversarial_input[0]\n","\n","  prediction = predict(model, input.cuda())\n","  attack_prediction = predict(model, adversarial_input.cuda())\n","\n","  if prediction == ground_truth:\n","    count += 1\n","\n","  if attack_prediction == ground_truth:\n","    attack_count += 1\n","\n","print(\"test accuracy\", count / total)\n","print(\"attack test accuracy\", attack_count / total)"],"execution_count":189,"outputs":[{"output_type":"stream","text":["test accuracy 1.0\n","attack test accuracy 0.02\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"60YRENqWnZ4C"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fI7D6XYjnZ6X","executionInfo":{"status":"ok","timestamp":1627961216326,"user_tz":240,"elapsed":119,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["import matplotlib.pyplot as plt\n","from IPython.display import Image \n","import os\n","import sys\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","DIR = \"/content/gdrive/MyDrive/omscs-dl-final-project\"\n","DATA_DIR = f'{DIR}/data'\n","NAME = 'cifar10'\n","DATA_SET_CLASS = torchvision.datasets.CIFAR10\n","image_size = 32\n","in_channels = 3\n","fgsm_alpha = 0.5\n","fgsm_epsilon = 0.01\n","\n","sys.path.append(os.path.abspath(DIR))\n","\n","import team36\n","from team36.mnist.vgg import VGG\n","from team36.attacks.fast_gradient_attack_data_set import FastSignGradientAttackDataSet\n","from team36.defenses.fast_gradient_sign_method_loss import FastGradientSignMethodLoss\n","from team36.training import validate, accuracy, predict"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbUpueZEnZ88","executionInfo":{"status":"ok","timestamp":1627961244959,"user_tz":240,"elapsed":124,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}}},"source":["fmodel = VGG(image_size=image_size, in_channels=in_channels)\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","state_dict = torch.load(f\"{DIR}/checkpoints/{NAME}-vgg.pth\")\n","model.load_state_dict(state_dict)\n","\n","defense_model = VGG(image_size=image_size, in_channels=in_channels)\n","if torch.cuda.is_available():\n","    defense_model = defense_model.cuda()\n","state_dict = torch.load(f\"{DIR}/checkpoints/{NAME}-vgg-training-with-fgsm-examples-defense-{fgsm_epsilon}.pth\")\n","defense_model.load_state_dict(state_dict)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","fgsm_reg_defense_model = VGG(image_size=image_size, in_channels=in_channels)\n","if torch.cuda.is_available():\n","    fgsm_reg_defense_model = fgsm_reg_defense_model.cuda()\n","state_dict = torch.load(f\"{DIR}/checkpoints/{NAME}-vgg-training-with-fgsm-regularization-defense-{fgsm_epsilon}.pth\")\n","fgsm_reg_defense_model.load_state_dict(state_dict)\n","\n","cpu_model = VGG(image_size=image_size, in_channels=in_channels)\n","state_dict = torch.load(f\"{DIR}/checkpoints/{NAME}-vgg.pth\")\n","cpu_model.load_state_dict(state_dict)\n","fgsm_reg_defense_criterion = FastGradientSignMethodLoss(cpu_model, nn.CrossEntropyLoss(), \n","                                                        alpha=fgsm_alpha, epsilon=fgsm_epsilon)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"OZif_es5nZ_g","executionInfo":{"status":"error","timestamp":1627961420866,"user_tz":240,"elapsed":1464,"user":{"displayName":"Eric Park","photoUrl":"","userId":"18321500450156250575"}},"outputId":"6693e316-772b-4485-b7f4-4a7e20d72a73"},"source":[" torch.cuda.empty_cache()\n","test_set = DATA_SET_CLASS(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())\n","\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n","\n","test_accuracy, _, test_loss = validate(None, test_loader, model, criterion)\n","defense_test_accuracy, _, defense_test_loss = validate(None, test_loader, defense_model, criterion)\n","fgsm_reg_defense_test_accuracy, _, fgsm_reg_defense_test_loss = validate(None, test_loader,\n","                                                                         fgsm_reg_defense_model, \n","                                                                         fgsm_reg_defense_criterion,\n","                                                                         no_grad=False)\n","\n","print(\"No Attack\")\n","print(f\"Test Accuracy is {test_accuracy}\")\n","print(f\"Test Loss is {test_loss}\")\n","\n","print(f\"FGSM Examples Defense Test Accuracy is {defense_test_accuracy}\")\n","print(f\"FGSM Examples Defense Test Loss is {defense_test_loss}\")\n","\n","print(f\"FGSM Regularization Defense Test Accuracy is {fgsm_reg_defense_test_accuracy}\")\n","print(f\"FGSM Regularization Defense Test Loss is {fgsm_reg_defense_test_loss}\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-157542ae4af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdefense_test_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefense_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefense_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m fgsm_reg_defense_test_accuracy, _, fgsm_reg_defense_test_loss = validate(None, test_loader,\n","\u001b[0;32m/content/gdrive/MyDrive/omscs-dl-final-project/team36/training.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(epoch, val_loader, model, criterion, no_grad)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/omscs-dl-final-project/team36/mnist/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 14.93 GiB already allocated; 29.75 MiB free; 14.95 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"XTTU9EZuAmCP"},"source":["test_set = DATA_SET_CLASS(root=DATA_DIR, train=False, download=True, transform=transforms.ToTensor())\n","attack_test_set = OnePixelAttackDataSet(test_set, model, criterion, epsilon=fgsm_epsilon, device='cuda')\n","attack_test_loader = torch.utils.data.DataLoader(attack_test_set, batch_size=100, shuffle=False)\n","\n","attack_test_accuracy, _, attack_test_loss = validate(None, attack_test_loader, model, criterion)\n","attack_defense_test_accuracy, _, attack_defense_test_loss = validate(None, attack_test_loader, \n","                                                                     defense_model, criterion)\n","attack_fgsm_reg_defense_test_accuracy, _, attack_fgsm_reg_defense_test_loss = validate(\n","    None, attack_test_loader, fgsm_reg_defense_model, fgsm_reg_defense_criterion, no_grad=False)\n","\n","\n","print(\"Attack\")\n","print(f\"Test Accuracy is {attack_test_accuracy}\")\n","print(f\"Test Loss is {attack_test_loss}\")\n","\n","print(f\"FGSM Examples Defense Test Accuracy is {attack_defense_test_accuracy}\")\n","print(f\"FGSM Examples Defense Test Loss is {attack_defense_test_loss}\")\n","\n","print(f\"FGSM Regularization Defense Test Accuracy is {attack_fgsm_reg_defense_test_accuracy}\")\n","print(f\"FGSM Regularization Defense Test Loss is {attack_fgsm_reg_defense_test_loss}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ib8YFjn_AmIA"},"source":["index = 1 # mnist\n","# index = 0 # cifar10\n","\n","input, ground_truth = test_set[index]\n","input = input.cuda()\n","\n","print(\"ground truth\", ground_truth)\n","\n","print(\"no attack\", predict(model, input))\n","display(transforms.functional.to_pil_image(input))\n","print()\n","\n","adversarial_input, _ = attack_test_set[index]\n","adversarial_input = adversarial_input.cuda()\n","\n","print(\"attack, no defense\", predict(model, adversarial_input))\n","display(transforms.functional.to_pil_image(adversarial_input))\n","print()\n","\n","print(\"attack, FGSM training defense\", predict(defense_model, adversarial_input))\n","display(transforms.functional.to_pil_image(adversarial_input))\n","print()\n","\n","print(\"attack, FGSM loss defense\", predict(fgsm_reg_defense_model, adversarial_input))\n","display(transforms.functional.to_pil_image(adversarial_input))\n","print()\n","\n","image_dir = f\"{DIR}/images\"\n","\n","pil_image = transforms.functional.to_pil_image(input)\n","pil_image.save(f\"{image_dir}/{NAME}-{str(index)}.png\")\n","\n","pil_image = transforms.functional.to_pil_image(adversarial_input)\n","pil_image.save(f\"{image_dir}/{NAME}-{str(index)}-{fgsm_epsilon}-fgsm-adversarial.png\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfey0aelAmLk"},"source":[""],"execution_count":null,"outputs":[]}]}